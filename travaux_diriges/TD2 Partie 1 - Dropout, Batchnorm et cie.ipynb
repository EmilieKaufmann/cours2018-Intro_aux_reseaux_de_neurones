{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xbf}{{\\bf x}}\n",
    "\\newcommand{\\ybf}{{\\bf y}}\n",
    "\\newcommand{\\wbf}{{\\bf w}}\n",
    "\\newcommand{\\Ibf}{\\mathbf{I}}\n",
    "\\newcommand{\\Xbf}{\\mathbf{X}}\n",
    "\\newcommand{\\Rbb}{\\mathbb{R}}\n",
    "\\newcommand{\\vec}[1]{\\left[\\begin{array}{c}#1\\end{array}\\right]}\n",
    "$\n",
    "\n",
    "# Introduction aux réseaux de neurones : TD #2  (partie 1)\n",
    "Matériel de cours rédigé par Pascal Germain, 2018\n",
    "************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation «Xavier» et «Kaiming»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.xavier_normal_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nn.Linear(5,5)\n",
    "L.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.xavier_uniform_(L.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.kaiming_normal_(L.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = nn.Dropout(p=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.ones(10)\n",
    "drop(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(drop(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = nn.Dropout(p=.75)\n",
    "for i in range(10):\n",
    "    print(drop(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop.eval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop.eval() # Mode prédiction\n",
    "for i in range(10):\n",
    "    print(drop(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop.train() # Mode apprentissage\n",
    "for i in range(10):\n",
    "    print(drop(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3,5),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Dropout())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage par «minibatch»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (torch.arange(1, 11, dtype=torch.float32) * torch.ones(3,10, dtype=torch.float32)).transpose(0,1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 10 * torch.arange(1, 11) \n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TensorDataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in data:\n",
    "    print(x, '<-->', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DataLoader(data, batch_size=3)\n",
    "for t in range(4):\n",
    "    print('******* EPOQUE', t)\n",
    "    for x, y in sampler:\n",
    "        print(x, '<-->', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DataLoader(data, batch_size=3, shuffle=True)\n",
    "for t in range(4):\n",
    "    print('******* EPOQUE', t)\n",
    "    for x, y in sampler:\n",
    "        print(x, '<-->', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation «batchnorm»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons un neurone. Lors de la phase de propagation de l'apprentissage, ce neurone est «traversé» par une minibatch de $m$ éléments au «temps» $t$.\n",
    "1.  Notons $a_{t;1}, \\ldots, a_{t;m}$ l'ensemble des valeurs sortie du neurone.\n",
    "2. On calcule \n",
    "$$\\begin{align*}\n",
    "\\mu_{a_t} &= \\frac1m \\sum_{i=1}^m a_{t;i}\\\\\n",
    "\\sigma_{a_t}  &= \\sqrt{\\epsilon + \\frac1m(\\mu_{a_t}  - a_{t;i} )^2} \\qquad \\mbox{($\\epsilon\\approx 10^{-8}$)}\n",
    "\\end{align*}$$\n",
    "3. Puis, on transforme chaque $a_i$:\n",
    "$$\\begin{align*}\\tilde a_k &= \\gamma_a \\frac{a_{t;i}-\\mu_{a_t} }{\\sigma_{a_t} } + \\beta_a,\\end{align*}$$\n",
    "où $\\gamma_a$ et $\\beta_a$ sont des paramètres associés au neurone au même titre que les poids, initialisés au hasard et appris par rétropropagation du gradient.\n",
    "\n",
    "$\\Rightarrow$ Lors de la prédiction, $\\gamma_a$ et $\\beta)a$ sont remplacés par des valeurs estimés sur tout l'ensemble d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.BatchNorm1d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = nn.BatchNorm1d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.weight # correspond au paramètre gamma_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.bias # correspond au paramètre beta_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DataLoader(data, batch_size=2, shuffle=False)\n",
    "for x, y in sampler:\n",
    "    print(x)\n",
    "    print(bn(x))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3,5),\n",
    "                      nn.Tanh(),\n",
    "                      nn.BatchNorm1d(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # Mode apprentissage\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Mode prédiction\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = nn.Softmax(dim=0)\n",
    "sm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(sm(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des raisons de stabilité numérique, *pyTorch* préfère travailler avec le logarithme de l'activation SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LogSoftmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsm = nn.LogSoftmax(dim=0) # Il  faut spécifier la dimension selon laquelle appliquer la normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(sm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "                nn.Linear(10, 3),\n",
    "                nn.LogSoftmax(dim=1) # Normalise chaque ligne\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(5, 10)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.exp().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perte du néfatif log vraissemblance associée à une sortie softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour nous faciliter la tâche, la fonction de perte `NLLLoss` est conçue pour gérer une paire d'arguments. Pour une minibatch de taille $m$ et un problème à $C$ classes, le calcul de la perte se fait à partir de:\n",
    "1. La prédiction donnée par une activation `LogSoftmax` (de taille $m\\times C$)\n",
    "2. La sortie désirée sous la forme d'un vecteur de $m$ éléments, chacun de ces éléments étant un entier de $0$ à $C-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.NLLLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perte = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.ones(5, dtype=torch.int64)\n",
    "perte(pred, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
