{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux réseaux de neurones : TD #2  -- Pistes de solutions\n",
    "Matériel de cours rédigé par Pascal Germain, 2018\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changer la taille et le nombre de filtres convolutifs\n",
    "\n",
    "Ici, nous réutilisons la classe `UneArchiPourMNIST` pour obtenir 64 filtres de taille 9 chacun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_archi = UneArchiPourMNIST(nb_filtres=64, taille_noyau=9)\n",
    "R = ReseauClassifGenerique(mon_archi, eta=0.1, alpha=0.1, nb_epoques=20, taille_batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter une ou plusieurs couches de filtres convolutifs dans la première partie du réseau\n",
    "\n",
    "Modifions la classe `UneArchiPourMNIST` afin d'ajouter une 2e couche de filtres  convolutifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archi2CouchesCovolutives:\n",
    "    def __init__(self, couche1_nb_filtres=16, couche1_taille_noyau=9, \n",
    "                       couche2_nb_filtres=32, couche2_taille_noyau=3):\n",
    "        # Créons deux couches de convolution \n",
    "        self.modele_conv = nn.Sequential(\n",
    "            # Première couche de convolution\n",
    "            nn.Conv2d(1, couche1_nb_filtres, kernel_size=couche1_taille_noyau),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # NOUVEAUTÉ: Deuxième couche de convolution\n",
    "            nn.Conv2d(couche1_nb_filtres, couche2_nb_filtres, kernel_size=couche2_taille_noyau),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # La convolution est suivie d'une couche de sortie\n",
    "        # NOUVEAUTÉ: Calcul du nombre de neurones sur la couche pleinement connectée\n",
    "        #            considérant la 2e couche de convolution\n",
    "        nb_pixels = (28 - couche1_taille_noyau + 1) // 2 \n",
    "        nb_pixels = (nb_pixels - couche2_taille_noyau + 1) // 2\n",
    "        self.nb_neurones_du_milieu = couche2_nb_filtres * (nb_pixels**2)\n",
    "        \n",
    "        self.modele_plein = nn.Sequential(\n",
    "            nn.Linear(self.nb_neurones_du_milieu, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def propagation(self, x, apprentissage=False):\n",
    "        # Propageons la «batch». Notez que nous devons redimensionner nos données consciencieusement\n",
    "        x0 = x.view(-1, 1, 28, 28)\n",
    "        x1 = self.modele_conv(x0)\n",
    "        x2 = x1.view(-1, self.nb_neurones_du_milieu)\n",
    "        x3 = self.modele_plein(x2)\n",
    "        return x3\n",
    "    \n",
    "    def parametres(self):\n",
    "        # Cette fonction doit retourner un tuple contenant toutes les variables à optimiser\n",
    "        return self.modele_conv.parameters(), self.modele_plein.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_archi = Archi2CouchesCovolutives(couche1_nb_filtres=16, couche1_taille_noyau=9,\n",
    "                                     couche2_nb_filtres=32, couche2_taille_noyau=3)\n",
    "R = ReseauClassifGenerique(mon_archi, eta=0.1, alpha=0.1, nb_epoques=20, taille_batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter davantage de couches pleinement connectées dans la seconde partie du réseau\n",
    "\n",
    "Reprenons l'architecture `Archi2CouchesCovolutives` ci-haut et ajoutons-y une couche cachée pleinement connectée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archi2CouchesCovolutives2CouchesPC:\n",
    "    def __init__(self, couche1_nb_filtres=16, couche1_taille_noyau=9, \n",
    "                       couche2_nb_filtres=32, couche2_taille_noyau=3,\n",
    "                       couche3_nb_neurones=100):\n",
    "        # Créons deux couches de convolution \n",
    "        self.modele_conv = nn.Sequential(\n",
    "            # Première couche de convolution\n",
    "            nn.Conv2d(1, couche1_nb_filtres, kernel_size=couche1_taille_noyau),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # Deuxième couche de convolution\n",
    "            nn.Conv2d(couche1_nb_filtres, couche2_nb_filtres, kernel_size=couche2_taille_noyau),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # La convolution est suivie d'une couche cachée pleinement connectée\n",
    "        nb_pixels = (28 - couche1_taille_noyau + 1) // 2\n",
    "        nb_pixels = (nb_pixels - couche2_taille_noyau + 1) // 2\n",
    "        self.nb_neurones_du_milieu = couche2_nb_filtres * (nb_pixels**2)\n",
    "        \n",
    "        self.modele_plein = nn.Sequential(\n",
    "            # NOUVEAUTÉ: Nouvelle couche cachée pleinement connectée avec activation ReLU\n",
    "            nn.Linear(self.nb_neurones_du_milieu, couche3_nb_neurones),  \n",
    "            nn.ReLU(),\n",
    "            # Couche de sortie\n",
    "            nn.Linear(couche3_nb_neurones, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def propagation(self, x, apprentissage=False):         \n",
    "        # Propageons la «batch». Notez que nous devons redimensionner nos données consciencieusement\n",
    "        x0 = x.view(-1, 1, 28, 28)\n",
    "        x1 = self.modele_conv(x0)\n",
    "        x2 = x1.view(-1, self.nb_neurones_du_milieu)\n",
    "        x3 = self.modele_plein(x2)\n",
    "        return x3\n",
    "    \n",
    "    def parametres(self):\n",
    "        # Cette fonction doit retourner un tuple contenant toutes les variables à optimiser\n",
    "        return self.modele_conv.parameters(), self.modele_plein.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_archi = Archi2CouchesCovolutives2CouchesPC(couche1_nb_filtres=16, couche1_taille_noyau=9, \n",
    "                                               couche2_nb_filtres=32, couche2_taille_noyau=3)\n",
    "R = ReseauClassifGenerique(mon_archi, eta=0.1, alpha=0.1, nb_epoques=20, taille_batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter du «Dropout»\n",
    "\n",
    "Reprenons l'architecture précedente et ajoutons du *Dropout* sur chacune des couches cachées. Notons qu'il est désormais essentiel de distinguer la phase **apprentissage** de la phase **évaluation** (voir les premières lignes de la  méthode `propagation`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchiDropout2CouchesCovolutives2CouchesPC:\n",
    "    def __init__(self, couche1_nb_filtres=16, couche1_taille_noyau=9, couche1_prob_dropout=0.5, \n",
    "                       couche2_nb_filtres=32, couche2_taille_noyau=3, couche2_prob_dropout=0.5, \n",
    "                       couche3_nb_neurones=100, couche3_prob_dropout=0.5):\n",
    "        # Créons deux couches de convolution \n",
    "        self.modele_conv = nn.Sequential(\n",
    "            # Première couche de convolution\n",
    "            nn.Conv2d(1, couche1_nb_filtres, kernel_size=couche1_taille_noyau),\n",
    "            nn.Dropout2d(couche1_prob_dropout), # <-- NOUVEAUTÉ\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "            # Deuxième couche de convolution\n",
    "            nn.Conv2d(couche1_nb_filtres, couche2_nb_filtres, kernel_size=couche2_taille_noyau),\n",
    "            nn.Dropout2d(couche2_prob_dropout),  # <-- NOUVEAUTÉ\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # La convolution est suivie d'une couche cachée pleinement connectée\n",
    "        nb_pixels = (28 - couche1_taille_noyau + 1) // 2\n",
    "        nb_pixels = (nb_pixels - couche2_taille_noyau + 1) // 2\n",
    "        self.nb_neurones_du_milieu = couche2_nb_filtres * (nb_pixels**2)\n",
    "        \n",
    "        self.modele_plein = nn.Sequential(\n",
    "            # Couche cachée pleinement connectée\n",
    "            nn.Linear(self.nb_neurones_du_milieu, couche3_nb_neurones),  \n",
    "            nn.Dropout(couche3_prob_dropout),  # <-- NOUVEAUTÉ\n",
    "            nn.ReLU(),\n",
    "            # Couche de sortie\n",
    "            nn.Linear(couche3_nb_neurones, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def propagation(self, x, apprentissage=False):\n",
    "        # Réseau avec «dropout»: il est essentiel de distinguer la phase d'apprentissage\n",
    "        # et la phase de prédiction!\n",
    "        if apprentissage:   \n",
    "            self.modele_conv.train()\n",
    "            self.modele_plein.train()\n",
    "        else:\n",
    "            self.modele_conv.eval()\n",
    "            self.modele_plein.eval()\n",
    "          \n",
    "        # Propageons la «batch». Notez que nous devons redimensionner nos données consciencieusement\n",
    "        x0 = x.view(-1, 1, 28, 28)\n",
    "        x1 = self.modele_conv(x0)\n",
    "        x2 = x1.view(-1, self.nb_neurones_du_milieu)\n",
    "        x3 = self.modele_plein(x2)\n",
    "        return x3\n",
    "    \n",
    "    def parametres(self):\n",
    "        # Cette fonction doit retourner un tuple contenant toutes les variables à optimiser\n",
    "        return self.modele_conv.parameters(), self.modele_plein.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparativement aux expérimentations précédentes, remarquez que nous avons diminué le taux d'apprentissage (paramètre `eta`) et augmenter le nombre d'époques (paramètre `nb_epoques`) après avoir constaté expérimentalement que l'ajout de *dropout* ralentie la convergence de la valeur objectif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_archi = ArchiDropout2CouchesCovolutives2CouchesPC(\n",
    "                    couche1_nb_filtres=16, couche1_taille_noyau=9, couche1_prob_dropout=0.5, \n",
    "                    couche2_nb_filtres=32, couche2_taille_noyau=3, couche2_prob_dropout=0.5, \n",
    "                    couche3_nb_neurones=100, couche3_prob_dropout=0.5\n",
    "                )\n",
    "R = ReseauClassifGenerique(mon_archi, eta=0.05, alpha=0.1, nb_epoques=40, taille_batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire de la «Batchnorm»\n",
    "\n",
    "Reprenons l'architecture précédente, en remplacant le *dropout* par la *batchnorm*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchiBatchnorm2CouchesCovolutives2CouchesPC:\n",
    "    def __init__(self, couche1_nb_filtres=16, couche1_taille_noyau=9, \n",
    "                       couche2_nb_filtres=32, couche2_taille_noyau=3,\n",
    "                       couche3_nb_neurones=100):\n",
    "        # Créons deux couches de convolution \n",
    "        self.modele_conv = nn.Sequential(\n",
    "            # Première couche de convolution\n",
    "            nn.Conv2d(1, couche1_nb_filtres, kernel_size=couche1_taille_noyau),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(couche1_nb_filtres),  # <-- NOUVEAUTÉ \n",
    "            nn.MaxPool2d(2),\n",
    "            # Deuxième couche de convolution\n",
    "            nn.Conv2d(couche1_nb_filtres, couche2_nb_filtres, kernel_size=couche2_taille_noyau),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(couche2_nb_filtres),  # <-- NOUVEAUTÉ\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "               \n",
    "        # La convolution est suivie d'une couche cachée pleinement connectée\n",
    "        nb_pixels = (28 - couche1_taille_noyau + 1) // 2\n",
    "        nb_pixels = (nb_pixels - couche2_taille_noyau + 1) // 2\n",
    "        self.nb_neurones_du_milieu = couche2_nb_filtres * (nb_pixels**2)\n",
    "        \n",
    "        self.modele_plein = nn.Sequential(\n",
    "            # Couche cachée pleinement connectée\n",
    "            nn.Linear(self.nb_neurones_du_milieu, couche3_nb_neurones),  \n",
    "            nn.BatchNorm1d(couche3_nb_neurones),  # <-- NOUVEAUTÉ\n",
    "            nn.ReLU(),\n",
    "            # Couche de sortie\n",
    "            nn.Linear(couche3_nb_neurones, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def propagation(self, x, apprentissage=False):\n",
    "        # Réseau avec «batchnorm»: il est essentiel de distinguer la phase d'apprentissage\n",
    "        # et la phase de prédiction!\n",
    "        if apprentissage: #  \n",
    "            self.modele_conv.train()\n",
    "            self.modele_plein.train()\n",
    "        else:\n",
    "            self.modele_conv.eval()\n",
    "            self.modele_plein.eval()\n",
    "          \n",
    "        # Propageons la «batch». Notez que nous devons redimensionner nos données consciencieusement\n",
    "        x0 = x.view(-1, 1, 28, 28)\n",
    "        x1 = self.modele_conv(x0)\n",
    "        x2 = x1.view(-1, self.nb_neurones_du_milieu)\n",
    "        x3 = self.modele_plein(x2)\n",
    "        return x3\n",
    "    \n",
    "    def parametres(self):\n",
    "        # Cette fonction doit retourner un tuple contenant toutes les variables à optimiser\n",
    "        return self.modele_conv.parameters(), self.modele_plein.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_archi = ArchiBatchnorm2CouchesCovolutives2CouchesPC(\n",
    "                    couche1_nb_filtres=16, couche1_taille_noyau=9,\n",
    "                    couche2_nb_filtres=32, couche2_taille_noyau=3, \n",
    "                    couche3_nb_neurones=100\n",
    "                )\n",
    "R = ReseauClassifGenerique(mon_archi, eta=0.05, alpha=0.1, nb_epoques=40, taille_batch=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
